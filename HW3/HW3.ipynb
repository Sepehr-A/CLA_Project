{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41263fbc-a85d-4e82-9527-4e20aab4c05f",
   "metadata": {},
   "source": [
    "# Inverse Power Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b0794-4d08-4b72-ae7b-c405dcb68999",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Inverse Power Method is an algorithm used in numerical linear algebra to find the eigenvalues and corresponding eigenvectors of a matrix. It is particularly useful for finding the eigenvalue of a matrix that is closest to a given number. This method is a variant of the Power Method, which is designed to find the eigenvalue of largest absolute value and its associated eigenvector. The Inverse Power Method, however, focuses on the smallest eigenvalue or those near a specific target value. It is particularly beneficial for dimensionality reduction, principal component analysis, and solving systems that require understanding of the underlying data structure through its eigenvectors and eigenvalues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d02869-a03c-40fc-b479-0918c74c854c",
   "metadata": {},
   "source": [
    "## Basic Idea\n",
    "The core idea behind the Inverse Power Method is to apply the Power Method to the inverse of the matrix $A$ (i.e. $A^{-1}$) rather than $A$ itself. Since the eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, applying the Power Method to $A^{-1}$ naturally focuses on the smallest eigenvalue of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a4d91",
   "metadata": {},
   "source": [
    "## Steps of the Inverse Power Method\n",
    "\n",
    "1. **Shift the Matrix (if necessary):** If a specific eigenvalue close to a known value $\\mu$ is sought, apply a shift to the matrix. This involves computing $B = A - \\mu I$, where $I$ is the identity matrix. This shift moves the eigenvalues of $A$ so that the one closest to $\\mu$ becomes the dominant eigenvalue of $B^{-1}$.\n",
    "\n",
    "2. **Choose an Initial Vector:** Select an initial guess vector $x_0$ that is not orthogonal to the desired eigenvector. This vector will be iteratively refined to approximate the eigenvector associated with the eigenvalue of interest.\n",
    "\n",
    "3. **Iterate:** At each iteration, solve $Bx_{k+1} = x_k$ for $x_{k+1}$, where $B = A - \\mu I$ (or $B = A$ if no shift is applied). This typically involves solving a system of linear equations, which can be computationally intensive but is manageable with modern algorithms and computing power.\n",
    "\n",
    "4. **Normalization:** After each iteration, normalize the vector $x_{k+1}$ to prevent numerical overflow or underflow.\n",
    "\n",
    "5. **Convergence Check:** Determine if the sequence of vectors $x_k$ is converging. This can be done by checking if the ratio $\\frac{|x_{k+1}|}{|x_k|}$ stabilizes, or if changes between successive vectors $x_k$ and $x_{k+1}$ are below a certain threshold.\n",
    "\n",
    "6. **Compute the Eigenvalue:** Once convergence is achieved, the eigenvalue $\\lambda$ closest to $\\mu$ can be approximated using the Rayleigh quotient, $\\lambda \\approx \\frac{x^T A x}{x^T x}$, where $x$ is the converged eigenvector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770f99a",
   "metadata": {},
   "source": [
    "# Deflation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41a9f3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Deflation Method is a technique used in numerical linear algebra to reduce the dimensionality of a problem, making it easier to find additional eigenvalues and eigenvectors of a matrix after the first few have already been found. It is particularly relevant when combined with methods like the Power Method or the Inverse Power Method, allowing these algorithms to iteratively find multiple eigenvalues and eigenvectors of a matrix. This approach is crucial in many data science applications where understanding the structure of data through its principal components or latent features is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec80467",
   "metadata": {},
   "source": [
    "## Deflation Process\n",
    "\n",
    "The Deflation Method typically involves modifying the original matrix $A$ after an eigenvalue $\\lambda$ and its corresponding eigenvector $v$ have been identified. The goal is to \"deflate\" the matrix, effectively reducing its dimension in a way that the already found eigenvalue does not influence the search for the next ones. There are several ways to perform deflation, with one common approach being to subtract from $A$ a rank-one matrix constructed from the outer product of the eigenvector $v$:\n",
    "\n",
    "$A' = A - \\lambda vv^T$\n",
    "\n",
    "where $v$ is normalized such that $v^T v = 1$. This operation reduces the influence of the found eigenvalue on $A'$, making it possible to apply the Inverse Power Method again to find the next eigenvalue and its corresponding eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef45b6",
   "metadata": {},
   "source": [
    "# Relationship Between Deflation and the Inverse Power Method\n",
    "\n",
    "The relationship between the Deflation Method and the Inverse Power Method becomes apparent when seeking to find more than the smallest eigenvalue (or those close to a target $\\mu$) of a matrix $A$. After applying the Inverse Power Method to find the smallest eigenvalue (or the one closest to $\\mu$), the deflation process adjusts the matrix $A$ so that the influence of this found eigenvalue and its corresponding eigenvector is \"removed,\" enabling the subsequent application of the Inverse Power Method to find the next eigenvalue and eigenvector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bcba3",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739304df",
   "metadata": {},
   "source": [
    "## Inverse Power Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f53a621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T09:16:17.714941700Z",
     "start_time": "2024-02-11T09:16:15.176883700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inverse_power_method(A, tol=1e-10, max_iterations=1000):\n",
    "    n = A.shape[0]\n",
    "    # Initial guess for the eigenvector\n",
    "    x = np.random.rand(n)\n",
    "    x = x / np.linalg.norm(x)  # Normalize the initial vector\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # Solve (A - mu*I)x_{k+1} = x_k for x_{k+1}. Here, we assume mu = 0 for simplicity\n",
    "        # This could be replaced with a shift if desired (A - mu*I)\n",
    "        y = np.linalg.solve(A, x)\n",
    "        \n",
    "        # Normalize y to get the next approximation of the eigenvector\n",
    "        x_next = y / np.linalg.norm(y)\n",
    "        \n",
    "        # Check for convergence (if the direction of x_next and x are not changing)\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        \n",
    "        x = x_next\n",
    "    \n",
    "    # Use the Rayleigh quotient to estimate the eigenvalue\n",
    "    eigenvalue = np.dot(x.T, A.dot(x)) / np.dot(x.T, x)\n",
    "    \n",
    "    return eigenvalue, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5e09a",
   "metadata": {},
   "source": [
    "- This implementation starts with a random initial guess for the eigenvector and normalizes it.\n",
    "\n",
    "- At each iteration, it solves the equation $Ax = \\lambda x$ by solving $A^{-1}y = x$ for $y$ (using `np.linalg.solve`), where $y$ becomes the next approximation of the eigenvector associated with the smallest eigenvalue.\n",
    "\n",
    "- The vector $y$ is normalized at each step to ensure numerical stability.\n",
    "\n",
    "- The algorithm checks if the change in direction of the eigenvector approximation between iterations is below a tolerance threshold, indicating convergence.\n",
    "\n",
    "- Once the eigenvector has been approximated, the corresponding eigenvalue is estimated using the Rayleigh quotient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34209da8",
   "metadata": {},
   "source": [
    "## Deflation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e3e2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T09:16:17.771472Z",
     "start_time": "2024-02-11T09:16:17.711952700Z"
    }
   },
   "outputs": [],
   "source": [
    "def deflate_symmetric_matrix(A, lambda1, v1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - A: The original symmetric matrix.\n",
    "    - lambda1: The eigenvalue to be deflated.\n",
    "    - v1: The corresponding normalized eigenvector of lambda1.\n",
    "    \n",
    "    Returns:\n",
    "    - A_deflated: The deflated matrix.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    A_deflated = np.copy(A)\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # Only need to iterate through half due to symmetry\n",
    "            A_deflated[i, j] -= lambda1 * v1[i] * v1[j]\n",
    "            if i != j:\n",
    "                A_deflated[j, i] = A_deflated[i, j]  # Ensure the matrix remains symmetric\n",
    "                \n",
    "    return A_deflated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2baac80",
   "metadata": {},
   "source": [
    "This function works by subtracting from each element of the matrix `A` the product of `lambda1`, the corresponding entry from the outer product of `v1` with itself, ensuring that the matrix remains symmetric. This adjustment effectively reduces the influence of the specified eigenvalue and its eigenvector, setting the stage for finding additional eigenpairs.\n",
    "\n",
    "Keep in mind that the example eigenvalue and eigenvector provided in this code are arbitrary and for demonstration purposes. In practice, you would use the eigenvalue and eigenvector obtained from applying an eigenvalue-finding algorithm like the Power Method, Inverse Power Method, or others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbdc2e-3635-44cc-951f-27fb840c2325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T18:22:05.127169100Z",
     "start_time": "2024-02-10T18:22:05.114169900Z"
    }
   },
   "source": [
    "# Test\n",
    "For testing our methods we use the following script to create a random matrix and then symmetrize it to ensure it's symmetric. This approach guarantees the matrix has real eigenvalues, which is suitable for our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bbe1d0-268e-42a7-b9a4-e44b975e2b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T09:16:17.798470900Z",
     "start_time": "2024-02-11T09:16:17.756943100Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_symmetric_matrix(size):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - size: The size of the matrix (number of rows & columns).\n",
    "    \n",
    "    Returns:\n",
    "    - A symmetric matrix with real eigenvalues.\n",
    "    \"\"\"\n",
    "    # Generate a random matrix\n",
    "    random_matrix = np.random.rand(size, size)\n",
    "    # Symmetrize the matrix to ensure it's symmetric\n",
    "    symmetric_matrix = (random_matrix + random_matrix.T) / 2\n",
    "    \n",
    "    return symmetric_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2c8587-f039-4873-bc08-bc076f5ea1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [-0.3140810548822642, 1.3805291406301775e-16]\n",
      "Eigenvector 1: [-0.10326257 -0.31853046  0.88054596 -0.33543107]\n",
      "Eigenvector 2: [ 0.10326257  0.31853046 -0.88054596  0.33543107]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate a symmetric matrix\n",
    "size = 4  # Size of the matrix\n",
    "A = create_symmetric_matrix(size)\n",
    "\n",
    "# Initialize lists to store eigenvalues and eigenvectors\n",
    "eigenvalues = []\n",
    "eigenvectors = []\n",
    "\n",
    "# Number of eigenpairs to find\n",
    "num_eigenpairs_to_find = 2\n",
    "\n",
    "for _ in range(num_eigenpairs_to_find):\n",
    "    # Step 2: Apply the inverse power method\n",
    "    lambda1, v1 = inverse_power_method(A)\n",
    "    \n",
    "    # Store the found eigenvalue and eigenvector\n",
    "    eigenvalues.append(lambda1)\n",
    "    eigenvectors.append(v1)\n",
    "    \n",
    "    # Step 3: Deflate the matrix\n",
    "    A = deflate_symmetric_matrix(A, lambda1, v1)\n",
    "\n",
    "# Display the found eigenvalues and eigenvectors\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "for i, vec in enumerate(eigenvectors, start=1):\n",
    "    print(f\"Eigenvector {i}:\", vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73db2a5-67c1-4b0e-b606-37fe079abe50",
   "metadata": {},
   "source": [
    "The first eigenvalue and eigenvector found represent the least dominant characteristic of the system described by \r",
    ".s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
